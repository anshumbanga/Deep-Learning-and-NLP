{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a52ef33b-2d9e-44fd-b7ea-5a32588aa05c",
   "metadata": {},
   "source": [
    "**Detailed Notes on Loss Functions in TensorFlow**\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### What is a Loss Function?\r\n",
    "\r\n",
    "A **loss function** in deep learning is a mathematical function that calculates the difference between the predicted output of the model and the actual target (true label or value). It guides the learning process by telling the model how wrong it is and how it should adjust its internal weights to improve.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## 1. Sparse Categorical Crossentropy\r\n",
    "\r\n",
    "```python\r\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\r\n",
    "```\r\n",
    "\r\n",
    "### When to Use:\r\n",
    "\r\n",
    "* Multi-class classification problems\r\n",
    "* Target labels are integers (e.g., 0, 1, 2, ...)\r\n",
    "\r\n",
    "### Example Use Case:\r\n",
    "\r\n",
    "* Classifying types of animals (cat=0, dog=1, horse=2) in an image\r\n",
    "\r\n",
    "### Explanation:\r\n",
    "\r\n",
    "It computes the cross-entropy loss between the true labels and the predicted probabilities. It is preferred when labels are not one-hot encoded.\r\n",
    "\r\n",
    "### Important Parameter:\r\n",
    "\r\n",
    "* `from_logits=True` if predictions are raw scores (logits)\r\n",
    "\r\n",
    "### Example:\r\n",
    "\r\n",
    "```python\r\n",
    "loss = SparseCategoricalCrossentropy(from_logits=False)\r\n",
    "y_true = [2]\r\n",
    "y_pred = [[0.1, 0.3, 0.6]]  # Predicted probabilities\r\n",
    "loss(y_true, y_pred).numpy()  # Output: 0.5108\r\n",
    "```\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## 2. Categorical Crossentropy\r\n",
    "\r\n",
    "```python\r\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\r\n",
    "```\r\n",
    "\r\n",
    "### When to Use:\r\n",
    "\r\n",
    "* Multi-class classification problems\r\n",
    "* Target labels are **one-hot encoded** (e.g., \\[0, 0, 1])\r\n",
    "\r\n",
    "### Example Use Case:\r\n",
    "\r\n",
    "* Handwritten digit classification (one-hot encoded digits)\r\n",
    "\r\n",
    "### Explanation:\r\n",
    "\r\n",
    "It calculates the cross-entropy between the one-hot encoded labels and predicted probabilities. Used when your dataset has categorical variables as output.\r\n",
    "\r\n",
    "### Example:\r\n",
    "\r\n",
    "```python\r\n",
    "loss = CategoricalCrossentropy()\r\n",
    "y_true = [[0, 0, 1]]\r\n",
    "y_pred = [[0.1, 0.3, 0.6]]\r\n",
    "loss(y_true, y_pred).numpy()  # Output: 0.5108\r\n",
    "```\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## 3. Binary Crossentropy\r\n",
    "\r\n",
    "```python\r\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\r\n",
    "```\r\n",
    "\r\n",
    "### When to Use:\r\n",
    "\r\n",
    "* Binary classification (output is 0 or 1)\r\n",
    "* Multi-label classification (each label is binary)\r\n",
    "\r\n",
    "### Example Use Case:\r\n",
    "\r\n",
    "* Spam email detection (spam=1, not spam=0)\r\n",
    "\r\n",
    "### Explanation:\r\n",
    "\r\n",
    "Calculates loss between binary true labels and predicted probabilities. Commonly used in logistic regression and binary classifiers.\r\n",
    "\r\n",
    "### Example:\r\n",
    "\r\n",
    "```python\r\n",
    "loss = BinaryCrossentropy()\r\n",
    "y_true = [[1], [0]]\r\n",
    "y_pred = [[0.9], [0.1]]\r\n",
    "loss(y_true, y_pred).numpy()  # Output: ~0.105\r\n",
    "```\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## 4. Mean Absolute Error (MAE)\r\n",
    "\r\n",
    "```python\r\n",
    "from tensorflow.keras.losses import MeanAbsoluteError\r\n",
    "```\r\n",
    "\r\n",
    "### When to Use:\r\n",
    "\r\n",
    "* Regression problems\r\n",
    "* You want equal weight for all errors\r\n",
    "\r\n",
    "### Example Use Case:\r\n",
    "\r\n",
    "* Predicting house prices\r\n",
    "\r\n",
    "### Explanation:\r\n",
    "\r\n",
    "It calculates the average of absolute differences between actual and predicted values. Less sensitive to outliers.\r\n",
    "\r\n",
    "### Example:\r\n",
    "\r\n",
    "```python\r\n",
    "loss = MeanAbsoluteError()\r\n",
    "y_true = [2.0, 4.0]\r\n",
    "y_pred = [2.5, 3.0]\r\n",
    "loss(y_true, y_pred).numpy()  # Output: 0.75\r\n",
    "```\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## 5. Mean Squared Error (MSE)\r\n",
    "\r\n",
    "```python\r\n",
    "from tensorflow.keras.losses import MeanSquaredError\r\n",
    "```\r\n",
    "\r\n",
    "### When to Use:\r\n",
    "\r\n",
    "* Regression problems\r\n",
    "* You want to penalize larger errors more heavily\r\n",
    "\r\n",
    "### Example Use Case:\r\n",
    "\r\n",
    "* Forecasting temperature, sales, or any continuous value\r\n",
    "\r\n",
    "### Explanation:\r\n",
    "\r\n",
    "It calculates the average of squared differences between predicted and actual values. Squaring the error increases the penalty for large deviations.\r\n",
    "\r\n",
    "### Example:\r\n",
    "\r\n",
    "```python\r\n",
    "loss = MeanSquaredError()\r\n",
    "y_true = [2.0, 4.0]\r\n",
    "y_pred = [2.5, 3.0]\r\n",
    "loss(y_true, y_pred).numpy()  # Output: 0.625\r\n",
    "```\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## Summary Comparison Table\r\n",
    "\r\n",
    "| Loss Function                 | Use Case                   | Label Type      | Notes                        |\r\n",
    "| ----------------------------- | -------------------------- | --------------- | ---------------------------- |\r\n",
    "| SparseCategoricalCrossentropy | Multi-class classification | Integer         | Faster & memory-efficient    |\r\n",
    "| CategoricalCrossentropy       | Multi-class classification | One-hot encoded | Needs categorical labels     |\r\n",
    "| BinaryCrossentropy            | Binary or multi-label      | 0 or 1          | Uses sigmoid; classification |\r\n",
    "| Mean Absolute Error (MAE)     | Regression                 | Float           | Robust to outliers           |\r\n",
    "| Mean Squared Error (MSE)      | Regression                 | Float           | Penalizes large errors more  |\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## Final Tip\r\n",
    "\r\n",
    "While compiling a model in TensorFlow:\r\n",
    "\r\n",
    "```python\r\n",
    "model.compile(\r\n",
    "    optimizer='adam',\r\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n",
    "    metrics=['accuracy']\r\n",
    ")\r\n",
    "```\r\n",
    "\r\n",
    "Use `from_logits=True` if the model does **not** have softmax or sigmoid activation at the output."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
